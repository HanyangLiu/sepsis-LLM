# Sepsis-LLM: Multimodal Deep Learning for Antimicrobial Resistance Prediction

A comprehensive machine learning framework for predicting antimicrobial resistance (AMR) in sepsis patients using multimodal clinical data and Large Language Models (LLMs).

## ğŸ¥ Overview

This project implements state-of-the-art multimodal deep learning models to predict antimicrobial resistance patterns in sepsis patients by integrating:

- **Static Data**: Demographics, admission details
- **Time-Series Data**: Vital signs and laboratory values over time  
- **Clinical Notes**: Free-text clinical documentation processed with medical LLMs
- **Comorbidity Codes**: ICD-10 diagnosis codes with learned embeddings


## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch lightning transformers scikit-learn pandas numpy
pip install catboost xgboost shap matplotlib seaborn
```

### Basic Usage

```bash
# Train the multimodal model with default configuration
python run.py --config configs/agg_mm.yaml --task AMR

# Run traditional ML baselines
python simple_ml.py

# Generate SHAP explanations
python analysis/evaluate_llm_shap.py
```

## ğŸ“Š Model Architecture

### AggMM (Aggregated Multimodal Model)

The core model (`AggMM`) uses a modular architecture:

```
â”Œâ”€â”€â”€ Static Encoder (FFN) â”€â”€â”€â”€â”
â”œâ”€â”€â”€ Time-Series Encoder â”€â”€â”€â”€â”¤â”€â”€â–º Fusion Layer â”€â”€â–º Prediction Head
â”œâ”€â”€â”€ Comorbidity Encoder â”€â”€â”€â”€â”¤    (Concatenation)     (Binary/Multi-class)
â””â”€â”€â”€ Clinical LLM Encoder â”€â”€â”€â”˜
```

**Key Components:**
- **Static Encoder**: Feed-forward network for demographic/admission data
- **Time-Series Encoder**: GRU-based recurrent network for vital signs/labs
- **Comorbidity Encoder**: Transformer with pretrained ICD-10 embeddings
- **Text Encoder**: Clinical LLM (BioGPT, Clinical-Longformer, etc.) for clinical notes

## ğŸ—‚ï¸ Project Structure

```
sepsis-LLM/
â”œâ”€â”€ configs/                    # Model configurations
â”‚   â””â”€â”€ agg_mm.yaml            # AggMM model config
â”œâ”€â”€ models/                    # Model implementations
â”‚   â”œâ”€â”€ _base_mm.py           # Base multimodal class
â”‚   â”œâ”€â”€ agg_mm.py             # Aggregated multimodal model
â”‚   â”œâ”€â”€ _text_encoder.py      # LLM integration
â”‚   â””â”€â”€ _modules.py           # Neural network modules
â”œâ”€â”€ analysis/                  # Analysis and evaluation tools
â”‚   â”œâ”€â”€ explain_llm.py        # LLM interpretability
â”‚   â”œâ”€â”€ evaluate_llm_shap.py  # SHAP explanations
â”‚   â”œâ”€â”€ missing_modality.py   # Robustness analysis
â”‚   â””â”€â”€ process_cohort_3_new/ # Data preprocessing
â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”œâ”€â”€ utils_data.py         # Data loading utilities
â”‚   â”œâ”€â”€ utils_evaluation.py   # Evaluation metrics
â”‚   â””â”€â”€ ontology/             # Medical ontologies
â”œâ”€â”€ run.py                     # Main training script
â”œâ”€â”€ experiment.py              # PyTorch Lightning experiment
â”œâ”€â”€ dataset_2.py              # Data loading and preprocessing
â”œâ”€â”€ simple_ml.py              # Traditional ML baselines
â””â”€â”€ paths.py                  # Data path configurations
```

## âš™ï¸ Configuration

The model behavior is controlled through YAML configuration files. Key parameters:

```yaml
model_params:
  name: 'AggMM'
  embed_size: 128
  llm_type: "microsoft/biogpt"  # Clinical LLM to use
  modalities: [True, True, True, False]  # [static, timeseries, comorbidity, notes]

data_params:
  task: "AMR"                   # AMR prediction or GNB detection
  note_type: "hpi"             # Clinical note section
  batch_size: 128
  use_precomputed: True        # Use cached LLM embeddings

exp_params:
  LR: 0.0001
  patience: 2
  max_epochs: 100
```

## ğŸ“ˆ Supported Tasks

### 1. Antimicrobial Resistance (AMR) Prediction
- **Binary**: Resistant vs. Susceptible
- **Multiclass**: Susceptible (SS) / Intermediate (RS) / Resistant (RR)

### 2. Gram-Negative Bacteria (GNB) Detection
- Predict presence of gram-negative bacteria in cultures

## ğŸ”¬ Clinical LLM Support

The framework supports multiple clinical language models:

| Model | Description | Use Case |
|-------|-------------|----------|
| **BioGPT** | Biomedical generative model | Long clinical narratives |
| **Clinical-Longformer** | Long-context clinical BERT | Extended clinical documents |
| **Bio_ClinicalBERT** | Clinical domain BERT | Standard clinical notes |
| **ClinicalBERT** | Alternative clinical BERT | General clinical text |


## ğŸ›ï¸ Advanced Usage

### Custom Model Development
```python
from models._base_mm import BaseMM

class CustomModel(BaseMM):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Custom architecture here
    
    def forward(self, batch, **kwargs):
        # Custom forward pass
        pass
```

### Custom Data Processing
```python
from dataset_2 import sepsisDataModule

# Custom data module with different preprocessing
dm = sepsisDataModule(
    max_codes=300,
    batch_size=64,
    note_type="assessment",  # Different note section
    infection_type="community",  # Specific infection type
    llm_type="emilyalsentzer/Bio_ClinicalBERT"
)
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
